{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Poem generator using LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amVnPqBbgORI"
      },
      "source": [
        "## Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhjWPIs2gORW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RNN\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWvjzQvmgORg"
      },
      "source": [
        "## Loading of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThlGesar3dsg"
      },
      "source": [
        "Here, we are loading a combined collection of all Shakespearean sonnets. I cleaned up this file to remove the start and end credits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "lEmnqQ8VgORk"
      },
      "source": [
        "text = (open(\"/content/peoms.txt\").read())\n",
        "text=text.lower()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm804UAVgORn"
      },
      "source": [
        "## Creating character/word mappings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lob3Unoy3miS"
      },
      "source": [
        "Now we will assign an arbitary number to each character. We are using character mapping here as our dataset is small and we have very few unique words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "nIcJ0_D1gORo"
      },
      "source": [
        "characters = sorted(list(set(text)))\n",
        "\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX-KhzlugORr"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNUk4WTR45ZM"
      },
      "source": [
        "Here, X is our train array, and Y is our target array.\n",
        "\n",
        "seq_length is the length of the sequence of characters that we want to consider before predicting a particular character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5KtXyvwAgORw"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "length = len(text)\n",
        "seq_length = 100\n",
        "\n",
        "for i in range(0, length-seq_length, 1):\n",
        "    sequence = text[i:i + seq_length]\n",
        "    label =text[i + seq_length]\n",
        "    X.append([char_to_n[char] for char in sequence])\n",
        "    Y.append(char_to_n[label])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUHDUQGD49cb"
      },
      "source": [
        "LSTMs accept input in the form of (number_of_sequences, length_of_sequence, number_of_features) which is not the current format of the arrays. Also, we need to transform the array Y into a one-hot encoded format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vPYYdnNrgOR2"
      },
      "source": [
        "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
        "X_modified = X_modified / float(len(characters))\n",
        "Y_modified = np_utils.to_categorical(Y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3R8i1M8gOR8"
      },
      "source": [
        "## A deeper model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rA_D-lGpgOSA"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(400, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(400, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(400))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS2aU5NpgOSE",
        "outputId": "4fdc2626-44d0-4d28-9140-45a799b4a6e0"
      },
      "source": [
        "model.fit(X_modified, Y_modified, epochs=100, batch_size=50)\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2003/2003 [==============================] - 132s 54ms/step - loss: 2.9188\n",
            "Epoch 2/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 2.3735\n",
            "Epoch 3/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 2.1843\n",
            "Epoch 4/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 2.0389\n",
            "Epoch 5/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.9332\n",
            "Epoch 6/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.8407\n",
            "Epoch 7/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.7588\n",
            "Epoch 8/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.6944\n",
            "Epoch 9/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.6258\n",
            "Epoch 10/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.5638\n",
            "Epoch 11/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.5110\n",
            "Epoch 12/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.4549\n",
            "Epoch 13/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.4014\n",
            "Epoch 14/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.3518\n",
            "Epoch 15/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.2995\n",
            "Epoch 16/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.2564\n",
            "Epoch 17/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.2188\n",
            "Epoch 18/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.1641\n",
            "Epoch 19/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.1233\n",
            "Epoch 20/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 1.0791\n",
            "Epoch 21/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 1.0490\n",
            "Epoch 22/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 1.0109\n",
            "Epoch 23/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.9813\n",
            "Epoch 24/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.9488\n",
            "Epoch 25/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.9320\n",
            "Epoch 26/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.9019\n",
            "Epoch 27/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.8715\n",
            "Epoch 28/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.8514\n",
            "Epoch 29/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.8229\n",
            "Epoch 30/100\n",
            "2003/2003 [==============================] - 108s 54ms/step - loss: 0.8052\n",
            "Epoch 31/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.8014\n",
            "Epoch 32/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.7651\n",
            "Epoch 33/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.7616\n",
            "Epoch 34/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.7446\n",
            "Epoch 35/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.7337\n",
            "Epoch 36/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.7248\n",
            "Epoch 37/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.7037\n",
            "Epoch 38/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.6945\n",
            "Epoch 39/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.6831\n",
            "Epoch 40/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.6805\n",
            "Epoch 41/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.6783\n",
            "Epoch 42/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.6568\n",
            "Epoch 43/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.6482\n",
            "Epoch 44/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.6477\n",
            "Epoch 45/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.6395\n",
            "Epoch 46/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.6301\n",
            "Epoch 47/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.6265\n",
            "Epoch 48/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.6171\n",
            "Epoch 49/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.6168\n",
            "Epoch 50/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.6099\n",
            "Epoch 51/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.6064\n",
            "Epoch 52/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.6060\n",
            "Epoch 53/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5933\n",
            "Epoch 54/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5886\n",
            "Epoch 55/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5865\n",
            "Epoch 56/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5837\n",
            "Epoch 57/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5895\n",
            "Epoch 58/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5813\n",
            "Epoch 59/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5771\n",
            "Epoch 60/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5726\n",
            "Epoch 61/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5682\n",
            "Epoch 62/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5656\n",
            "Epoch 63/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5616\n",
            "Epoch 64/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5549\n",
            "Epoch 65/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5703\n",
            "Epoch 66/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5552\n",
            "Epoch 67/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5550\n",
            "Epoch 68/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5646\n",
            "Epoch 69/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5513\n",
            "Epoch 70/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5625\n",
            "Epoch 71/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5565\n",
            "Epoch 72/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5481\n",
            "Epoch 73/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5581\n",
            "Epoch 74/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5520\n",
            "Epoch 75/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5386\n",
            "Epoch 76/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5491\n",
            "Epoch 77/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5384\n",
            "Epoch 78/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5397\n",
            "Epoch 79/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5549\n",
            "Epoch 80/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5396\n",
            "Epoch 81/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5485\n",
            "Epoch 82/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5391\n",
            "Epoch 83/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5359\n",
            "Epoch 84/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5395\n",
            "Epoch 85/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5344\n",
            "Epoch 86/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5376\n",
            "Epoch 87/100\n",
            "2003/2003 [==============================] - 109s 54ms/step - loss: 0.5382\n",
            "Epoch 88/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5372\n",
            "Epoch 89/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5423\n",
            "Epoch 90/100\n",
            "2003/2003 [==============================] - 110s 55ms/step - loss: 0.5319\n",
            "Epoch 91/100\n",
            "2003/2003 [==============================] - 110s 55ms/step - loss: 0.5297\n",
            "Epoch 92/100\n",
            "2003/2003 [==============================] - 110s 55ms/step - loss: 0.5342\n",
            "Epoch 93/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5318\n",
            "Epoch 94/100\n",
            "2003/2003 [==============================] - 110s 55ms/step - loss: 0.5355\n",
            "Epoch 95/100\n",
            "2003/2003 [==============================] - 110s 55ms/step - loss: 0.5361\n",
            "Epoch 96/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5218\n",
            "Epoch 97/100\n",
            "2003/2003 [==============================] - 110s 55ms/step - loss: 0.5248\n",
            "Epoch 98/100\n",
            "2003/2003 [==============================] - 110s 55ms/step - loss: 0.5367\n",
            "Epoch 99/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5334\n",
            "Epoch 100/100\n",
            "2003/2003 [==============================] - 109s 55ms/step - loss: 0.5301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa19d799ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVTr3hyLhFuA"
      },
      "source": [
        "model.save_weights('text_generator_400_0.2_400_0.2_400_0.2_100.h5')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywzlbMJTgOSI"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO84-A5J5XTr"
      },
      "source": [
        "We take random row from the X array, that is an array of 100 characters. After this, we target predicting another 100 characters following X. The input is reshaped and scaled as previously and the next character with maximum probability is predicted.\n",
        "\n",
        "seq is used to store the decoded format of the string that has been predicted till now. Next, the new string is updated, such that the first character is removed and the new predicted character is included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Cr2HHjzNgOSJ"
      },
      "source": [
        "string_mapped = X[99]\n",
        "full_string = [n_to_char[value] for value in string_mapped]\n",
        "# generating characters\n",
        "for i in range(400):\n",
        "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
        "    x = x / float(len(characters))\n",
        "\n",
        "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
        "    seq = [n_to_char[value] for value in string_mapped]\n",
        "    full_string.append(n_to_char[pred_index])\n",
        "\n",
        "    string_mapped.append(pred_index)\n",
        "    string_mapped = string_mapped[1:len(string_mapped)]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUm1TxPZgOSM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6f1538c1-5a06-4145-b46a-d15a421def07"
      },
      "source": [
        "#combining text\n",
        "txt=\"\"\n",
        "for char in full_string:\n",
        "    txt = txt+char\n",
        "txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n  but as the riper should by time decease,\\n  his tender heir might bear his memory:\\n  but thou, control of the thing they wart,\\n  who hateth thee that the world have stand thence:\\n  and yet that be besper'd that heart's part,\\n  still constant in a wondrous excellence;\\n  therefore are feasts so shape which it doth latch:\\n  of his quick objects hath the mind exes,\\n  for they in thee to thy beauty show\\n  that wh'touls poor inc masgents love   which thould transport me farthest from your sight.\\n  b\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAH4Hzt66Rk3"
      },
      "source": [
        "Our generated peom has sensible words and it also rythms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "TGjkuR3ygOSO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}